{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title, smoothing = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): List of results arrays to plot\n",
    "        legend_list (list): List of legends corresponding to each result array\n",
    "        color_list (list): List of color corresponding to each result array\n",
    "        ylabel (string): Label of the vertical axis\n",
    "\n",
    "        Make sure the elements in the arr_list, legend_list, and color_list\n",
    "        are associated with each other correctly (in the same order).\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # Set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the vertical labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # Plot results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # Compute the standard error (of raw data, not smoothed)\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # Plot the mean\n",
    "        averages = moving_average(arr.mean(axis=0)) if smoothing else arr.mean(axis=0)\n",
    "        h, = ax.plot(range(arr.shape[1]), averages, color=color, label=legend)\n",
    "        # Plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), averages - arr_err, averages + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # Save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # Plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car racing A2C NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "\n",
    "        # Common CNN layers for feature extraction\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(4096, 512)  # Adjust based on input dimensions\n",
    "        self.common_activation = nn.ReLU()\n",
    "\n",
    "        # Actor network\n",
    "        self.actor_fc = nn.Linear(512, 3)  # Outputs mean of actions\n",
    "        self.actor_std = nn.Parameter(torch.ones(1, 3))  # Learnable standard deviation\n",
    "\n",
    "        # Critic network\n",
    "        self.critic_fc = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.contiguous().view(x.size(0), -1)  # Ensure tensor is contiguous before flattening\n",
    "        # print(x.shape)\n",
    "        x = self.common_activation(self.fc1(x))\n",
    "\n",
    "        # Actor: mean and standard deviation\n",
    "        action_mean = torch.tanh(self.actor_fc(x))\n",
    "        action_std = torch.exp(self.actor_std)\n",
    "\n",
    "        # Critic: state value\n",
    "        state_value = self.critic_fc(x)\n",
    "\n",
    "        return action_mean, action_std, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Car racing A2C agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n",
    "    def __init__(self):\n",
    "        self.policy_net = ActorCriticNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).permute(0, 3, 1, 2) / 255.0\n",
    "        action_mean, action_std, state_value = self.policy_net(state_tensor)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return action.squeeze(0).detach().numpy(), log_prob, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgentTrainer:\n",
    "    def __init__(self, agent, env, params):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.params = params\n",
    "        self.gamma = params['gamma']\n",
    "        self.optimizer = torch.optim.Adam(self.agent.policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    def rollout(self):\n",
    "        self.saved_rewards = []\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_state_values = []\n",
    "\n",
    "        state, _ = self.env.reset()\n",
    "        is_done = False\n",
    "\n",
    "        while not is_done:\n",
    "            action, log_prob, state_value = self.agent.get_action(state)\n",
    "            next_state, reward, done, trunc, _ = self.env.step(action)\n",
    "            is_done = done or trunc\n",
    "\n",
    "            self.saved_rewards.append(reward)\n",
    "            self.saved_log_probs.append(log_prob)\n",
    "            self.saved_state_values.append(state_value)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    def update_agent_policy_network(self):\n",
    "        returns = deque()\n",
    "        g_return = 0\n",
    "\n",
    "        # Calculate returns\n",
    "        for reward in reversed(self.saved_rewards):\n",
    "            g_return = reward + self.gamma * g_return\n",
    "            returns.appendleft(g_return)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32)\n",
    "        policy_loss = []\n",
    "        value_loss = []\n",
    "\n",
    "        for log_prob, value, ret in zip(self.saved_log_probs, self.saved_state_values, returns):\n",
    "            advantage = ret - value\n",
    "            policy_loss.append(-log_prob * advantage)\n",
    "            value_loss.append(advantage.pow(2))\n",
    "\n",
    "        total_loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        del self.saved_rewards[:]\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_state_values[:]\n",
    "\n",
    "        return returns[0].item(), total_loss.item()\n",
    "\n",
    "    def train(self):\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "        ep_bar = tqdm.trange(self.params['num_episodes'])\n",
    "\n",
    "        for ep in ep_bar:\n",
    "            self.rollout()\n",
    "            G, loss = self.update_agent_policy_network()\n",
    "\n",
    "            train_returns.append(G)\n",
    "            train_losses.append(loss)\n",
    "            ep_bar.set_description(f\"Episode: {ep} | Return: {G:.2f} | Loss: {loss:.2f}\")\n",
    "\n",
    "        return train_returns, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 160 | Return: 3.63 | Loss: -8054.37:  16%|█▌        | 161/1000 [40:09<4:07:16, 17.68s/it]  "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CarRacing-v3', continuous=True, render_mode=None)\n",
    "    params = {\n",
    "        'num_episodes': 1000,\n",
    "        'learning_rate': 1e-4,\n",
    "        'gamma': 0.99\n",
    "    }\n",
    "\n",
    "    agent = ActorCriticAgent()\n",
    "    trainer = ActorCriticAgentTrainer(agent, env, params)\n",
    "    returns, losses = trainer.train()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(agent.policy_net.state_dict(), \"car_racing_actor_critic.pth\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20e40d8fc09a6690434ad602c7eb2d8de15d36ec466bfbfb0de97c7c540d7363"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
